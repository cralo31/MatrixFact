% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/main.R
\name{nmf.main}
\alias{nmf.main}
\title{Algorithms for Nonnegative Matrix Factorization}
\usage{
nmf.main(X, mode = 1, k, method = "nmf", init = "random",
  iter = 200, tol = 0.00001, tau = 0.1, step_bin = 0.05,
  step_log = 0.001, factor = 2, sparse_svd = TRUE, seed = 0)
}
\arguments{
\item{X}{Matrix; An n-by-p matrix with either continous or binary entries.}

\item{mode}{Integer; Define the type of the input matrix. 1 for continuous, 2 for binary.}

\item{k}{Clusters; Number of clusters / factors to solve for. Must not exceed the minimum of n or p.}

\item{method}{String; Determine which NMF algorithm to use. Choices include 'nmf', 'onmf', 'semi', 'sonmf' for
continuous matrices and "so_bin", "log_bin" for binary matrices. Defaults to 'nmf'.}

\item{init}{String; Determine the type of initialization to use. Choices include 'random', 'kmeans', and 'svd'. Defaults to 'random'.}

\item{iter}{Integer; Number of iterations to run. Defaults to 200.}

\item{tol}{Double; Stop the algorithm when the difference between two iterations is less than this specified 
threshold. Defaults to 1e-5.}

\item{tau}{Double; Initial step size for the line search algorithm in sonmf. Defaults to 0.5.}

\item{step_bin}{Double; Step size for the update algorithm in binary SONMF. Defaults to 0.05.}

\item{step_log}{Double; Step size for both gradient descent updates for logNMF. Defaults to 0.001. This value should be tuned with caution,
as convergence performance is rather unstable. Recommend to leave it as default.}

\item{factor}{Double; The factor in which the step size in sonmf is increased/decreased during the line search. Defaults to 2.}

\item{sparse_svd}{Boolean; Determine whether to use the exact SVD decomposition from \code{svd()} or the fast-truncated SVD 
from \code{irlba()} for 'svd' initialization.}

\item{seed}{Integer; Set seed for reproducibility. Defaults to no seed set.}
}
\value{
A \code{MatrixFact} object; a list consisting of
\item{F}{The final F matrix}
\item{G}{The final G matrix}
\item{info}{A table with the tolerance, averaged residual, and orthogonal residual(if applicable) at each iteration}
\item{final_res}{A vector with the final factorized residual and orthogonal residual(if applicable) and the number
of iterations}
}
\description{
Function to apply various methods of NMF on the input matrix for both continuous and binary entries.
}
\details{
The Non-negative Matrix Factorization aims to factorize/approximate a target matrix X as the product of two lower-rank
matrices, F and G.
}
\examples{

### Create an arbitrary 100-by-100 non-negative matrix to factorize. ###

# Run Regular NMF with random initialization #

n = 100 
X = matrix(rnorm(n * n, 0, 1), n, n)
X[X < 0] = 0
mode = 1
k = 10
method = "nmf"
init = "random"
iter = 200
tol = 1e-5

result.1 = nmf.main(X, mode, k, method, init, iter, tol)

# Run SONMF with SVD initialization using the same X as above #

method = "sonmf"
init = "svd"

result.2 = nmf.main(X, mode, k, method, init, iter, tol)

# Run binary SONMF with SVD initialization for binary X.

n = 100
X = matrix(rbinom(n^2, 1, runif(n^2, 0.25, 0.75)), n, n)  
mode = 2
k = 10
method = "so_bin"
init = "svd"
iter = 200
tol = 1e-6

result.3 = nmf.main(X, mode, k, method, init, iter, tol)

}
\references{
Lee, D. D., & Seung, H. S. (2001). Algorithms for non-negative matrix factorization. In Advances in neural information processing systems (pp. 556-562).
DOI: \url{http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization}.

Ding, C. H., Li, T., & Jordan, M. I. (2010). Convex and semi-nonnegative matrix factorizations. IEEE transactions on pattern analysis and machine intelligence, 32(1), 45-55.
DOI: \url{http://dx.doi.org/10.1109/TPAMI.2008.277}

Wen, Z. and Yin, W., "A feasible method for optimization with orthogonality constraints." Mathematical Programming 142.1-2 (2013): 397-434.
DOI: \url{https://doi.org/10.1007/s10107-012-0584-1}.

Kimura, K., Tanaka, Y., & Kudo, M. (2015, February). A fast hierarchical alternating least squares algorithm for orthogonal nonnegative matrix factorization. In Asian Conference on Machine Learning (pp. 129-141).
DOI: \url{http://proceedings.mlr.press/v39/kimura14.pdf}

TomÃ©, A. M., Schachtner, R., Vigneron, V., Puntonet, C. G., & Lang, E. W. (2015). A logistic non-negative matrix factorization approach to binary data sets. Multidimensional Systems and Signal Processing, 26(1), 125-143.
DOI: \url{https://doi.org/10.1007/s11045-013-0240-9}

Li, J. Y., Zhu, R., Qu, A., Ye, H., & Sun, Z. (2018). Semi-Orthogonal Non-Negative Matrix Factorization. arXiv preprint arXiv:1805.02306.
DOI: \url{https://arxiv.org/abs/1805.02306}
}
